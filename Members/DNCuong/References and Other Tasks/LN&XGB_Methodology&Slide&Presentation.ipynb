{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Multiple Linear Regression in Bitcoin Price Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Methodology for MLR\n",
    "\n",
    "Multiple Linear Regression is a statistical technique that aims to predict the value of a dependent variable based on multiple independent variables. The step-by-step process for implementing Multiple Linear Regression for Bitcoin price prediction is outlined below.\n",
    "\n",
    "1. Data preprocessing methodology work with Time-series data:\n",
    "- This data is time series it's sequential, so we don't use Cross-Validation or any of the model ML techniques to evaluate error. TimeSeriesSplit, which is a specific type of cross-validation technique used for time series data. It's important to use time-series cross-validation when dealing with sequential data to avoid training on future data. TimeSeriesSplit splits the data into folds, so that the folds with data from the previous past will be used as the training set, and the future data will only be used as the test set. For example, if we split the data into 3 folds, each fold would consist of:\n",
    "    - Fold 1: Data from January 2016 to December 2017 (training set) and data from January 2018 to December 2018 (test set).    \n",
    "    - Fold 2: Data from January 2016 to December 2018 (training set) and data from January 2019 to December 2019 (test set).    \n",
    "    - Fold 3: Data from January 2016 to December 2019 (training set) and data from January 2020 to December 2020 (test set). <br>\n",
    "\n",
    "The code: `tscv = TimeSeriesSplit(n_splits=3)` will creat a time-series cross-validation object that splits the data into 3 folds in chronological order. \n",
    "\n",
    "2. Linear Regression methodology: \n",
    "\n",
    "2.1 Definition\n",
    "- Linear Regression methodology:\n",
    "    - Simple Linear Regression: only 1 independent var Y = b0 + b1*X\n",
    "    - Multi Linear Regression: More than one independent variable   \n",
    "    Y = m0 + m1X1 + m2X2 + m3X3 + ... + mNXN\n",
    "    - Polynomial Regression: independent variable of higher order than 1 (for example, order 2, 3)\n",
    "- Logistic Reg: Classification Problem. (probability prediction of dependent variable based on independent variables.)\n",
    "\n",
    "2.2 How Linear Regression Model works:\n",
    "- In Linear Regression, we need to find the parameter set m_i (i from 0 to N) so that the function Y = f(X) best fits the training data set.\n",
    "\n",
    "- The process of finding the model parameter: w0, w1, ..., wn set through the training process can use Gradient Descent, Stochastic Gradient Descent or Normal Equation methods by optimizing the MSE loss function. \n",
    "\n",
    "MSE (Mean Squared Error) or loss function formula (between the predicted values and the true values): \n",
    "\n",
    " $$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    " \n",
    "\n",
    "3. Lasso and Ridge Regression\n",
    "\n",
    "3.1.1 Definition: \n",
    "- Lasso and Ridge Regression are two regularization methods used to reduce overfitting in Linear Regression model.    \n",
    "    - Lasso uses L1 regularization: to remove unimportant variables completely BY push the coefficients to zero completely. \n",
    "    - Ridge uses L2 regularization: to remove unimportant variables incompletely BY push the coefficients close to zero, but never down to zero completely.    \n",
    "- The choice between these two methods depends on THE NUMBER OF FEATURES AND INFLUENCE OF EACH FEATURE.\n",
    "\n",
    "3.1.2. How model work? similar to Linear Regression with loss function. \n",
    "\n",
    "And the formula for Lasso Regression loss function is:\n",
    "\n",
    "$$MSE_{Lasso} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^{p} |w_j| = MSE + \\alpha \\sum_{j=1}^{p} |w_j|$$  \n",
    "\n",
    "The formula for Ridge Regression loss function is:\n",
    "\n",
    "$$MSE_{Ridge} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^{p} w_j^2 = MSE + \\alpha \\sum_{j=1}^{p} w_j^2$$  \n",
    "\n",
    "- $MSE$ is the mean squared error between the predicted values and the true values.\n",
    "- $n$ is the number of training samples.\n",
    "- $p$ is the number of features in the model.\n",
    "- $y_i$ is the true value of the ith sample.\n",
    "- $\\hat{y_i}$ is the predicted value of the ith sample.\n",
    "- $w_j$ is the weight corresponding to the jth feature in the model.\n",
    "- $\\alpha$ is the regularization parameter.\n",
    "<br>\n",
    "\n",
    "3. 2 Packaging the LinearRegressionModel Class for hyperparameters tuning in Ridge and Lasso Regression.\n",
    "\n",
    "Grid Search (tìm kiếm theo lưới) and randomized search (tìm kiếm ngẫu nhiên) for hyperparameters tuning:\n",
    "- are approaches to finding hyperparameters for machine learning models.\n",
    "- Grid Search:    \n",
    "    - Make sure to find the best solution.    \n",
    "    - Easy reproducibility of results.    \n",
    "    - It takes a lot of computation time when the model has many hyperparameters.\n",
    "- Randomized Search in reverse. (fits multiple hyperparameters)\n",
    "\n",
    "- 3. 3 Important Features\n",
    "\n",
    "- Important Features methodology: \n",
    "\n",
    "To identify important features and improve model performance, we can follow a few methodologies. Experiment with features: \n",
    "\n",
    "- 1.  Remove features(that are not important or can cause interference). \n",
    "\n",
    "- 2. Testing feature combinations(combine features together to create new features) \n",
    "\n",
    "- 3. Consider selecting a different model(if the linear model is not powerful enough for complex relationships between variables).\n",
    "\n",
    "- 4. Combine important features with other models.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Idea Slide for MLR + Pictures\n",
    "\n",
    "1. Linear Regression:\n",
    "    - Simple Linear Regression: only 1 independent var Y = b0 + b1*X\n",
    "    - Multi Linear Regression: More than one independent variable   \n",
    "    Y = m0 + m1X1 + m2X2 + m3X3 + ... + mNXN\n",
    "\n",
    "MSE (Mean Squared Error) loss function: mean squared error between the predicted values and the true values.\n",
    "\n",
    "Linear Regression loss function is:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "2. Lasso and Ridge Regression: \n",
    "\n",
    "Lasso Regression loss function is:\n",
    "\n",
    "$$MSE_{Lasso} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^{p} |w_j|$$\n",
    "\n",
    "Ridge Regression loss function is:\n",
    "\n",
    "$$MSE_{Ridge} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^{p} w_j^2$$\n",
    "\n",
    "\n",
    "\n",
    "- $w_j$ is the weight corresponding to the jth feature in the model.\n",
    "- $\\alpha$ is the regularization parameter.\n",
    "<br>\n",
    "\n",
    "3. Important Features\n",
    "\n",
    "Picture: \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Presentation Script for MLR: (with Result of Model)\n",
    "\n",
    "1. Definition: \n",
    "\n",
    "- Multiple Linear Regression is a statistical technique that aims to predict the value of a dependent variable based on multiple independent variables.\n",
    "\n",
    "We need to make a little distinction between Simple Linear Regression and Mutiple Linear Regression\n",
    "\n",
    "    - Simple Linear Regression: only 1 independent variable Y = b0 + b1*X \n",
    "    \n",
    "In there: \n",
    "- `y` is the dependent variable\n",
    "- `X` is the independent variable\n",
    "- `b0` is the intercept (also known as the coefficient of freedom or constant term)\n",
    "- `b1` is the coefficient or the slope of the regression line\n",
    "\n",
    "    - Multiple Linear Regression: More than one independent variable   \n",
    "    Y = m0 + m1X1 + m2X2 + m3X3 + ... + mNXN**   \n",
    "In there, y, X1 to XN, m0 to mN are defined in the same way as in Simple Linear Regression.\n",
    "\n",
    "2. How Model works:\n",
    "- In Linear Regression, we need to find the parameter set m_i (i from 0 to N) so that the function Y = f(X) best fits the training data set.\n",
    "\n",
    "- The process of finding the model parameter: w0, w1, ..., wn set through the training process can use Gradient Descent, Stochastic Gradient Descent or Normal Equation methods by optimizing the MSE loss function. \n",
    "\n",
    "This is MSE (Mean Squared Error) or loss function formula (between the predicted values and the true values): \n",
    "\n",
    " $$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "<!-- Đọc công thức này bằng tiếng anh: MSE equal one over n, multiple ... -->\n",
    "\"The Mean Squared Error (MSE) is equal to one over n, multiplied by the sum of the squared differences between the actual values (y) and the predicted values (ŷ).\n",
    "\n",
    "In there:\n",
    "- Yi is the actual output value of the i-th data point;\n",
    "- Ŷi is the output value predicted by the model with input Xi;\n",
    "- n is the number of data points in the training set.\n",
    "\n",
    "\n",
    "3. Lasso and Ridge Regression\n",
    "- Due to reduce overfitting in Linear Regression model we use Lasso and Ridge Regression\n",
    "    - Lasso uses L1 regularization: to remove unimportant variables completely BY push the coefficients to zero completely. \n",
    "    - Ridge uses L2 regularization: to remove unimportant variables incompletely BY push the coefficients close to zero, but never down to zero completely.    \n",
    "- The choice between these two methods depends on THE NUMBER OF FEATURES AND INFLUENCE OF EACH FEATURE.\n",
    "\n",
    "3.2. How model work? similar to Linear Regression with loss function. \n",
    "\n",
    "And the formula for Lasso Regression loss function is:\n",
    "\n",
    "$$MSE_{Lasso} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^{p} |w_j| = MSE + \\alpha \\sum_{j=1}^{p} |w_j|$$  \n",
    "\n",
    "The MSE of Lasso equals the MSE plus the regularization parameter alpha multiplied by the sum of the absolute values of the model coefficients (w_j).\n",
    "\n",
    "The formula for Ridge Regression loss function is:\n",
    "\n",
    "$$MSE_{Ridge} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2 + \\alpha \\sum_{j=1}^{p} w_j^2 = MSE + \\alpha \\sum_{j=1}^{p} w_j^2$$  \n",
    "\n",
    "The MSE of Ridge equals the MSE plus the regularization parameter alpha multiplied by the sum of squares of the model coefficients (w_j).\n",
    "\n",
    "In there: \n",
    "- $MSE$ is the mean squared error between the predicted values and the true values.\n",
    "- $n$ is the number of training samples.\n",
    "- $p$ is the number of features in the model.\n",
    "- $y_i$ is the true value of the ith sample.\n",
    "- $\\hat{y_i}$ is the predicted value of the ith sample.\n",
    "- $w_j$ is the weight corresponding to the jth feature in the model.\n",
    "- $\\alpha$ is the regularization parameter.\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - XGBoost Regression (Extreme Gradient Boosting) in Bitcoin Price Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Methodology for XGB Regression\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful machine learning algorithm that is widely used for regression and classification tasks. It is an ensemble learning method that combines the predictions of multiple weak learners to create a strong predictive model. Here is the step-by-step methodology for implementing XGBoost Regression for Bitcoin price prediction:\n",
    "\n",
    "1. Data preprocessing:\n",
    "- Splitting the dataset: Since this is a time-series dataset, it is crucial to split the data chronologically, ensuring that the training data comes before the testing data.\n",
    "\n",
    "2. XGBoost Regression model methodology:\n",
    "\n",
    "2.1 Definition: \n",
    "- Ensemble Methods: method of combining multiple models (search gg Machine Learning picture)    \n",
    "    - Stacking techniques    \n",
    "    - Bagging technique: Random Forest alg.  \n",
    "    - Boosting technique: XGBoost, LightGBM, CatBoost, AdaBoost. Repeat the training process with different weights.        \n",
    "        - XGBoost: regularization to reduce overfitting, gradient boosting to optimize tree weights, parallelization to train independent trees and speed up the training process, tools to monitor (giám sát) the training process.        \n",
    "        - AdaBoost is the first boosting algorithm, CatBoost focuses on feature categorical (tính phân loại) and LightGBM focuses on speed and scalability for large datasets.\n",
    "\n",
    "- Definition XGBoost:         \n",
    "XGBoost is a machine learning algorithm based on a decision tree and gradient boosting technique, designed to handle continuous classification or estimation problems, efficient and accurate in processing the data. large and multidimensional data sets.\n",
    "\n",
    "2.2 How does the XGBoost model work? \n",
    "\n",
    "- The goal of the XGBoost model is to optimize the loss function, minimizing the difference between the predicted value and the actual value.\n",
    "- XGBoost use: regularization to reduce overfitting, gradient boosting to optimize tree weights, parallelization to train independent trees and speed up the training process, tools to monitor (giám sát) the training process.  \n",
    "- The loss function of XGBoost is the sum of the loss functions of each tree in the population (quần thể), including the loss function of the regularization terms(thuật ngữ chính quy).\n",
    "\n",
    "The main formula of the loss function in XGBoost is: \n",
    "\n",
    "$$Obj^{(t)}=\\sum_{i=1}^{n}l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_i))+\\Omega(f_t) $$\n",
    "\n",
    "In there:\n",
    "- $Obj^{(t)}$ is the loss function value at the t_th loop.\n",
    "- $n$ is the number of training data points.\n",
    "- $y_i$ is the target value of the i-th data point.\n",
    "- $\\hat{y}_i^{(t-1)}$ is the predicted value of the model at the previous loop t-1.\n",
    "- $f_t(x_i)$ is the predicted value of the t tree on the ith data point.\n",
    "- $l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_i))$ is a loss function, which measures the difference between the predicted value and the actual value economy on the i-th data point.\n",
    "- $\\Omega(f_t)$ is the regularization term (hàm chi phí), which measures the complexity of the t-th tree.\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Idea Slide for XGB Regression + Pictures\n",
    "\n",
    "1. Ensemble Methods: \n",
    "- Ensemble Methods: method of combining multiple models (search gg Machine Learning picture)    \n",
    "    - Stacking techniques    \n",
    "    - Bagging technique: Random Forest alg.  \n",
    "    - Boosting technique: XGBoost, LightGBM, CatBoost, AdaBoost. Repeat the training process with different weights.        \n",
    "      \n",
    "\n",
    "2. XGBoost: \n",
    "\n",
    "- Definition XGBoost:         \n",
    "XGBoost is a machine learning algorithm based on decision tree and gradient boosting technique, designed to handle continue classification or estimation problems, efficient and accurate in processing the data. large and multidimensional data sets.\n",
    "\n",
    "- XGBoost use: regularization to reduce overfitting, gradient boosting to optimize tree weights, parallelization to train independent trees and speed up the training process, tools to monitor (giám sát) the training process.  \n",
    "\n",
    "The main formula of the loss function in XGBoost is: \n",
    "\n",
    "$$Obj^{(t)}=\\sum_{i=1}^{n}l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_i))+\\Omega(f_t)$$\n",
    "\n",
    "In there:\n",
    "- $Obj^{(t)}$ is the loss function value at the t_th loop.\n",
    "- $n$ is the number of training data points.\n",
    "- $y_i$ is the target value of the i-th data point.\n",
    "- $\\hat{y}_i^{(t-1)}$ is the predicted value of the model at the previous loop t-1.\n",
    "- $f_t(x_i)$ is the predicted value of the t tree on the ith data point.\n",
    "- $l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_i))$ is a loss function, which measures the difference between the predicted value and the actual value economy on the i-th data point.\n",
    "- $\\Omega(f_t)$ is the regularization term (hàm chi phí), which measures the complexity of the t-th tree.\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Presentation Script for XGB Regression (with Result of Model)\n",
    "\n",
    "0. Ensemble Methods is a method of combining multiple models. Three common ensemble methods are Stacking, Bagging and Boosting techniques.\n",
    "\n",
    "1. Definition XGBoost:         \n",
    "- XGBoost (eXtreme Gradient Boosting) is a machine learning algorithm based on decision tree and gradient boosting technique, designed to handle continue classification or estimation problems, efficient and accurate in processing the data large and multidimensional data sets.\n",
    "\n",
    "2. How does the XGBoost model work? \n",
    "- The goal of the XGBoost model is to optimize the loss function, minimizing the difference between the predicted value and the actual value.\n",
    "\n",
    "- XGBoost use: regularization to reduce overfitting, gradient boosting to optimize tree weights, parallelization to train independent trees and speed up the training process, tools to monitor (giám sát) the training process.  \n",
    "- The loss function of XGBoost is the sum of the loss functions of each tree in the population (quần thể), including the loss function of the regularization terms(thuật ngữ chính quy).\n",
    "\n",
    "The main formula of the loss function in XGBoost is: \n",
    "\n",
    "$$Obj^{(t)}=\\sum_{i=1}^{n}l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_i))+\\Omega(f_t) $$\n",
    "\n",
    "\"Objective value at iteration t $Obj^{(t)}$ is equal to the sum of the loss function ($l$) evaluated for each training data point $i$,\n",
    " where the loss function compares the target value $(y_i)$ \n",
    " with the predicted value at the previous iteration $\\hat{y}_i^{(t-1)}$ plus the predicted value of the t-th tree $f_t(x_i)$,\n",
    "  and this sum is then added to the regularization term $\\Omega(f_t)$.\"\n",
    "\n",
    "\n",
    "In there:\n",
    "- $Obj^{(t)}$ is the loss function value at the t_th loop.\n",
    "- $n$ is the number of training data points.\n",
    "- $y_i$ is the target value of the i-th data point.\n",
    "- $\\hat{y}_i^{(t-1)}$ is the predicted value of the model at the previous loop t-1.\n",
    "- $f_t(x_i)$ is the predicted value of the t tree on the ith data point.\n",
    "- $y_i, \\hat{y}_i^{(t-1)}$ is\n",
    "- $l(y_i, \\hat{y}_i^{(t-1)}+f_t(x_i))$ is a loss function,\n",
    " which measures the difference between the predicted value and the actual value economy on the i-th data point.\n",
    "- $\\Omega(f_t)$ is the regularization term (hàm chi phí), which measures the complexity of the t-th tree."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
