{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Các kiểu mạng nơ-ron:\n",
    "\n",
    "- ANN (Artificial Neural Network): đơn giản phân loại, dự đoán. \n",
    "- RNN (Recurrent Neural Network) Mạng nơ-ron hồi quy: Các biến thể Long Short-Term Memory (LSTM) và Gated Recurrent Unit (GRU)\n",
    "- CNN (Convolutional Neural Network): hình ảnh, video \n",
    "- GNN (Graph Neural Network): đồ thị \n",
    "- GAN (Generative Adversarial Network) Mạng nơ-ron sinh tạo: 2 mạng nơ-ron đối địch 'mạng sinh' và 'mạng phân biệt'\n",
    "- Transformer Mạng nơ-ron biến đổi: cơ chế self-attention - ngôn ngữ tự nhiên\n",
    "- Autoencoder Mạng nơ-ron tự sao chép: encoder-decoder rút trích đặc trưng, nén data\n",
    "\n",
    "- ANN không phù hợp trong dự đoán chuỗi thời gian phức tạp như Bitcoin. \n",
    "https://github.com/PiSimo/BitcoinForecast   (GRU)\n",
    "https://github.com/abhinavsagar/cryptocurrency-price-prediction/blob/master/README.md (LSTM)\n",
    "https://github.com/buianhkiet110703/TIME-SERIES?fbclid=IwAR0Thp_E6ddoxz50x5fgBSw_C2uSig7e9EljrfcFz2yadIl-m-SqDQtWOiU (Time-series and LSTM)\n",
    "\n",
    "https://github.com/NishkarshRaj/100DaysofMLCode/blob/master/12_GraphNeuralNetwork/GNN.ipynb\n",
    "https://github.com/NishkarshRaj/100DaysofMLCode/tree/master/9_Deep_Learning\n",
    "\n",
    "- trong phần này mình sẽ dùng CNN: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GIẢI THÍCH CÁCH HOẠT ĐỘNG CỦA SELF- ATTENTION.\n",
    "Các mô hình ngôn ngữ lớn (LLMs) đang làm cách mạng hóa thế giới trí tuệ nhân tạo, và Attention đã trở thành một công cụ rất quan trọng trong đó.\n",
    "self-attention là một kỹ thuật quan trọng được sử dụng để xử lý các chuỗi đầu vào. Kỹ thuật self-attention cho phép mô hình tập trung vào các phần quan trọng của chuỗi và học được các phụ thuộc xa giữa các phần tử trong chuỗi.\n",
    "Trong LLMs,self-attention được sử dụng để học các trọng số cho mỗi từ trong câu dựa trên tất cả các từ khác trong câu đó. Các trọng số này được sử dụng để tính tổng trọng số của các vectơ từ. Kết quả của tổng trọng số này được sử dụng để tạo ra một vectơ đại diện cho toàn bộ câu.\n",
    "Từ đó, các mô hình LLMs sử dụng các vectơ đại diện này để thực hiện các tác vụ như dự đoán từ tiếp theo trong câu hoặc dịch máy. Các mô hình LLMs hiện nay như GPT (Generative Pre-trained Transformer), BERT (Bidirectional Encoder Representations from Transformers) và Transformer sử dụng self-attention để học các phụ thuộc xa giữa các phần tử trong chuỗi đầu vào và tạo ra các dự đoán chính xác hơn."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date  BTC_close  BTC_open  BTC_high   BTC_low  BTC_volume   \n",
      "0     2010-07-20   0.000000  0.000000  0.000000  0.000002    0.000090  \\\n",
      "1     2010-07-21   0.000000  0.000000  0.000000  0.000002    0.000410   \n",
      "2     2010-07-22   0.000000  0.000000  0.000000  0.000002    0.001991   \n",
      "3     2010-07-23   0.000000  0.000000  0.000000  0.000002    0.002231   \n",
      "4     2010-07-24   0.000000  0.000000  0.000000  0.000002    0.000330   \n",
      "...          ...        ...       ...       ...       ...         ...   \n",
      "4649  2023-04-12   0.442578  0.447362  0.441697  0.447419    0.078570   \n",
      "4650  2023-04-13   0.449997  0.442661  0.442438  0.450208    0.065742   \n",
      "4651  2023-04-14   0.451257  0.449992  0.448827  0.452643    0.098273   \n",
      "4652  2023-04-15   0.448697  0.451253  0.443342  0.455398    0.031560   \n",
      "4653  2023-04-16   0.448855  0.448685  0.442745  0.454280    0.034332   \n",
      "\n",
      "      Active_Addr_Cnt  Difficulty  Mean_Block_Size(in_bytes)   \n",
      "0            0.000387         0.0                   0.000266  \\\n",
      "1            0.000275         0.0                   0.000074   \n",
      "2            0.000136         0.0                   0.000062   \n",
      "3            0.000181         0.0                   0.000045   \n",
      "4            0.000403         0.0                   0.000435   \n",
      "...               ...         ...                        ...   \n",
      "4649         0.773109         1.0                   0.762550   \n",
      "4650         0.738798         1.0                   0.714667   \n",
      "4651         0.743463         1.0                   0.693927   \n",
      "4652         0.765144         1.0                   0.725619   \n",
      "4653         0.615323         1.0                   0.736159   \n",
      "\n",
      "      Sum_Block_Weight  ...       ETH       LTC      DOGE       XRP      GOLD   \n",
      "0             0.000667  ...  0.000000  0.000000  0.000000  0.000000  0.139341  \\\n",
      "1             0.000285  ...  0.000000  0.000000  0.000000  0.000000  0.139439   \n",
      "2             0.000157  ...  0.000000  0.000000  0.000000  0.000000  0.143165   \n",
      "3             0.000169  ...  0.000000  0.000000  0.000000  0.000000  0.135517   \n",
      "4             0.001140  ...  0.000000  0.000000  0.000000  0.000000  0.135517   \n",
      "...                ...  ...       ...       ...       ...       ...       ...   \n",
      "4649          0.837243  ...  0.398592  0.237888  0.121416  0.181558  0.942636   \n",
      "4650          0.747626  ...  0.418459  0.243498  0.127160  0.184331  0.972446   \n",
      "4651          0.737169  ...  0.436733  0.249056  0.129145  0.188018  0.934105   \n",
      "4652          0.831951  ...  0.434781  0.249884  0.129411  0.186799  0.934105   \n",
      "4653          0.652828  ...  0.440749  0.258596  0.131704  0.187371  0.947245   \n",
      "\n",
      "        SILVER    COPPER    S&P500       DJI     JP225  \n",
      "0     0.160779  0.000000  0.009671  0.009105  0.050664  \n",
      "1     0.163766  0.000000  0.005966  0.005024  0.049703  \n",
      "2     0.172374  0.656057  0.012389  0.012549  0.047129  \n",
      "3     0.171858  0.657509  0.014787  0.016365  0.056461  \n",
      "4     0.171858  0.657509  0.014787  0.016365  0.056461  \n",
      "...        ...       ...       ...       ...       ...  \n",
      "4649  0.371630  0.835348  0.812071  0.882433  0.885056  \n",
      "4650  0.384310  0.848651  0.826545  0.896721  0.888355  \n",
      "4651  0.371684  0.845372  0.824257  0.891312  0.903304  \n",
      "4652  0.371684  0.845372  0.824257  0.891312  0.903304  \n",
      "4653  0.371086  0.845372  0.824257  0.891312  0.903304  \n",
      "\n",
      "[4654 rows x 24 columns]\n",
      "Index(['Date', 'BTC_close', 'BTC_open', 'BTC_high', 'BTC_low', 'BTC_volume',\n",
      "       'Active_Addr_Cnt', 'Difficulty', 'Mean_Block_Size(in_bytes)',\n",
      "       'Sum_Block_Weight', 'Total_Fees(USD)', 'Mean_Hashrate', 'Xfer_Cnt',\n",
      "       'Mean_Tx_size(native_units)', 'ETH', 'LTC', 'DOGE', 'XRP', 'GOLD',\n",
      "       'SILVER', 'COPPER', 'S&P500', 'DJI', 'JP225'],\n",
      "      dtype='object')\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "import pandas as pd\n",
    "# data = pd.read_csv(\"https://raw.githubusercontent.com/lavibula/ML20222.PredictionBitcoin/main/data/MinMax_Normalized_data.csv\")\n",
    "data = pd.read_csv(r\"C:\\Users\\Administrator\\OneDrive - Hanoi University of Science and Technology\\ITE10 - Data Science and AI - HUST\\20222\\ML\\Source_Codes\\ML20222.PredictionBitcoin\\data\\MinMax_Normalized_data.csv\")\n",
    "data = data.drop(['Unnamed: 0'], axis=1)\n",
    "print(data)\n",
    "print(data.columns)\n",
    "print(len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.drop(['BTC_close', 'Date'], axis=1)\n",
    "y = data['BTC_close']\n",
    "# Chia train, valid, test: 1-... = 0.75, (0.15/0.85)*0.8=0.2, 02\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=False)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.15/0.85, random_state=42, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Mạng nơ-ron biến đổi: cơ chế self-attention - ngôn ngữ tự nhiên\n",
    "Transformer là một kiến trúc mạng nơ-ron sử dụng cho xử lý ngôn ngữ tự nhiên, đặc biệt trong các bài toán dịch máy. Cơ chế self-attention (tự chú ý) là điểm nhấn của kiến trúc này. Thay vì sử dụng kiến trúc Recurrent Neural Network (RNN) truyền thống, Transformer sử dụng self-attention để giải quyết vấn đề việc thông tin xa nhau không được liên kết tốt, và đồng thời cũng giải quyết vấn đề việc xử lý các chuỗi đầu vào dài, mà không gặp phải vấn đề gradient vanishing hoặc exploding.\n",
    "\n",
    "## Autoencoder Mạng nơ-ron tự sao chép: encoder-decoder rút trích đặc trưng, nén data\n",
    "Autoencoder là một kiến trúc mạng nơ-ron được sử dụng để học các đặc trưng tiềm ẩn của dữ liệu. Nó bao gồm hai phần chính: encoder và decoder. Encoder được sử dụng để rút trích đặc trưng từ dữ liệu đầu vào và giảm chiều của dữ liệu, trong khi decoder thực hiện chuyển đổi dữ liệu xuống chiều thấp hơn và tạo ra một bản sao của dữ liệu đầu vào. Autoencoder rất hữu ích trong việc giảm chiều của dữ liệu và thực hiện nén dữ liệu để lưu trữ, giảm tải bộ nhớ và xử lý dữ liệu, đồng thời cũng có thể được sử dụng để tạo ra các đặc trưng mới từ dữ liệu ban đầu.\n",
    "\n",
    "Có thể sử dụng cả Transformer và Autoencoder để học và dự đoán giá Bitcoin tương lai, nhưng cần phải thực hiện các bước tiền xử lý dữ liệu và chọn kiến trúc mạng phù hợp cho bài toán.\n",
    "\n",
    "Đầu tiên, cần phân chia dữ liệu thành tập huấn luyện và tập kiểm tra. Sau đó, áp dụng một số kỹ thuật tiền xử lý như chuẩn hóa, rời rạc hóa hoặc smoothing để làm giảm giao động giá trị của Bitcoin.\n",
    "\n",
    "Sau khi đã tiền xử lý dữ liệu và xây dựng được tập huấn luyện và tập kiểm tra, có thể sử dụng Transformer để học các mẫu trong dữ liệu Bitcoin theo ngày và dự đoán giá trị tương lai. Tuy nhiên, để áp dụng Transformer cho bài toán này, cần biểu diễn dữ liệu Bitcoin theo một chuỗi liên tục các vector (embedding). Các embedding này được học từ dữ liệu đã được tiền xử lý và được đưa vào mô hình Transformer để dự đoán giá trị trong tương lai.\n",
    "\n",
    "Ngoài ra, Autoencoder cũng có thể được sử dụng để giảm chiều dữ liệu Bitcoin và rút trích các đặc trưng tiềm ẩn từ dữ liệu. Sau đó, các đặc trưng này có thể được đưa vào một mô hình dự đoán giá trị Bitcoin như mạng nơ-ron feedforward.\n",
    "\n",
    "Tuy nhiên, cần lưu ý rằng việc dự đoán giá trị Bitcoin trong tương lai là một bài toán khó và không đảm bảo độ chính xác cao. Cần sử dụng nhiều phương pháp và kiến trúc mạng khác nhau để tăng độ chính xác của mô hình dự đoán."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Train Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để sử dụng mạng Transformer trong việc dự đoán giá Bitcoin với tập dữ liệu trên, bạn cần tiến hành các bước sau:\n",
    "\n",
    "Mạng Transformer là một kiến trúc mạng nơ-ron trí tuệ nhân tạo (AI) được sử dụng rộng rãi trong các miền xử lý ngôn ngữ tự nhiên (NLP), nhưng cũng có thể được sử dụng trong các bài toán dự đoán chuỗi thời gian. Để xây dựng mô hình, bạn có thể sử dụng các thư viện deep learning như PyTorch hoặc TensorFlow. \n",
    "\n",
    "Để sử dụng mạng Transformer trong TensorFlow cho bài toán dự đoán giá Bitcoin, bạn cần thực hiện các bước sau:\n",
    "\n",
    "1. Chuẩn bị dữ liệu: Tiền xử lý dữ liệu để chuẩn hóa và phù hợp với đầu vào của mô hình.\n",
    "\n",
    "2. Xây dựng kiến trúc mô hình: Sử dụng API xây dựng mô hình từ TensorFlow để tạo các lớp biến đổi (transformer layers) và kết nối chúng để tạo thành kiến trúc mô hình Transformer.\n",
    "\n",
    "3. Huấn luyện mô hình: Tiến hành huấn luyện mô hình thông qua việc sử dụng các thuật toán tối ưu hoá trên tập dữ liệu đã được chuẩn bị.\n",
    "\n",
    "4. Đánh giá và finetune: Đánh giá mô hình và tinh chỉnh để cải thiện độ chính xác bằng cách sử dụng các phương pháp như regularizations, dropout, …\n",
    "\n",
    "Cách tiếp cận này khá tương tự với cách làm trong PyTorch, tuy nhiên có một số khác biệt về cú pháp và các tính năng phức tạp hơn của TensorFlow. Ngoài ra, PyTorch được đánh giá cao về khả năng debug và quản lý tính toán trong lúc huấn luyện, trong khi TensorFlow được sử dụng phổ biến hơn trong các mô hình lớn và ứng dụng thực tế."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_features, n_head=8, n_hidden=512, n_layers=6, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Define the model type and mask\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "\n",
    "        # Create the encoder layers\n",
    "        self.pos_encoder = PositionalEncoding(n_features, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(n_features, n_head, n_hidden, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, n_layers)\n",
    "\n",
    "        # Add linear layers for input and output\n",
    "        self.encoder = nn.Linear(n_features, n_features)\n",
    "        self.decoder = nn.Linear(n_features, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # Generate a mask for the source sequence\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        # Apply positional encoding and pass through the transformer encoder\n",
    "        src = self.encoder(src)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "\n",
    "        # Pass the final encoded sequence through the decoder linear layer\n",
    "        output = self.decoder(output[:, -1, :])\n",
    "        return output\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        '''Generate a square mask for the sequence.'''\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Initialize the positional encoding table\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add the positional encoding to the input and apply dropout\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là một triển khai của mô hình Transformer sử dụng PyTorch để thực hiện các tác vụ dự đoán với đầu vào chuỗi thời gian.\n",
    "\n",
    "Mô hình được xây dựng bằng lớp `TransformerModel`, kế thừa từ lớp `nn.Module` trong PyTorch. Các thuộc tính của mô hình bao gồm:\n",
    "- `model_type`: xác định loại mô hình (trong trường hợp này là \"Transformer\").\n",
    "- `src_mask`: là mặt nạ được tạo ra để ẩn đi các phần không cần thiết trong chuỗi đầu vào.\n",
    "\n",
    "Lớp `PositionalEncoding` được sử dụng để giúp mô hình học được thông tin về vị trí của các token trong chuỗi. Mô hình sẽ sử dụng ma trận positional encoding `pe` để biến đổi đầu vào $x$ và áp dụng dropout sau đó.\n",
    "\n",
    "Mô hình sử dụng lớp `nn.TransformerEncoder` để mã hóa đầu vào, sau đó chuyển qua lớp `nn.Linear` để giải mã đầu ra của mô hình.\n",
    "\n",
    "Hàm `_generate_square_subsequent_mask()` được sử dụng để tạo ra mặt nạ cho đầu vào là một đối tượng `sz` với shape là `(sz, sz)`.\n",
    "\n",
    "Hàm `forward()` thực hiện quá trình lấy đầu vào `src`, tạo mặt nạ cho đầu vào (nếu cần), sử dụng positional encoding và pass qua transformer encoder. Sau đó, kết quả sau khi đã được encodex được trích xuất để đưa qua giải mã lineer, trả về kết quả dự đoán.\n",
    "\n",
    "Các tham số được sử dụng trong mô hình bao gồm:\n",
    "- `n_features`: số lượng feature của input.\n",
    "- `n_head`: số lượng head trong multi-head attention.\n",
    "- `n_hidden`: số lượng hidden unit trong fully connected layer.\n",
    "- `n_layers`: số lượng layer trong encoder.\n",
    "- `dropout`: tỷ lệ dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Triển khai self-attention bằng Pytorch \n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.mm import functional as F\n",
    "\n",
    "# class SelfAttention(nn.Module): \n",
    "#     \"Single head of self-attention\"\n",
    "#     def __init__(self, head-size): \n",
    "\n",
    "#     def forward(self,x): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "embed_dim must be divisible by num_heads",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m test_loader \u001b[39m=\u001b[39m DataLoader(test_dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m     32\u001b[0m \u001b[39m# Define the model, criterion, optimizer, and scheduler\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m model \u001b[39m=\u001b[39m TransformerModel(n_features\u001b[39m=\u001b[39;49mX\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\n\u001b[0;32m     34\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\n\u001b[0;32m     35\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 15\u001b[0m, in \u001b[0;36mTransformerModel.__init__\u001b[1;34m(self, n_features, n_head, n_hidden, n_layers, dropout)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39m# Create the encoder layers\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpos_encoder \u001b[39m=\u001b[39m PositionalEncoding(n_features, dropout)\n\u001b[1;32m---> 15\u001b[0m encoder_layers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mTransformerEncoderLayer(n_features, n_head, n_hidden, dropout)\n\u001b[0;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer_encoder \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTransformerEncoder(encoder_layers, n_layers)\n\u001b[0;32m     18\u001b[0m \u001b[39m# Add linear layers for input and output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:445\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.__init__\u001b[1;34m(self, d_model, nhead, dim_feedforward, dropout, activation, layer_norm_eps, batch_first, norm_first, device, dtype)\u001b[0m\n\u001b[0;32m    443\u001b[0m factory_kwargs \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m: device, \u001b[39m'\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m'\u001b[39m: dtype}\n\u001b[0;32m    444\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m--> 445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mself_attn \u001b[39m=\u001b[39m MultiheadAttention(d_model, nhead, dropout\u001b[39m=\u001b[39;49mdropout, batch_first\u001b[39m=\u001b[39;49mbatch_first,\n\u001b[0;32m    446\u001b[0m                                     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs)\n\u001b[0;32m    447\u001b[0m \u001b[39m# Implementation of Feedforward model\u001b[39;00m\n\u001b[0;32m    448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear1 \u001b[39m=\u001b[39m Linear(d_model, dim_feedforward, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:969\u001b[0m, in \u001b[0;36mMultiheadAttention.__init__\u001b[1;34m(self, embed_dim, num_heads, dropout, bias, add_bias_kv, add_zero_attn, kdim, vdim, batch_first, device, dtype)\u001b[0m\n\u001b[0;32m    967\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39m=\u001b[39m batch_first\n\u001b[0;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m=\u001b[39m embed_dim \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m num_heads\n\u001b[1;32m--> 969\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim \u001b[39m*\u001b[39m num_heads \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39m\"\u001b[39m\u001b[39membed_dim must be divisible by num_heads\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_qkv_same_embed_dim:\n\u001b[0;32m    972\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_proj_weight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((embed_dim, embed_dim), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mAssertionError\u001b[0m: embed_dim must be divisible by num_heads"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "X_train_valid, X_test, y_train_valid, y_test = train_test_split(X, y, test_size=0.15, random_state=42, shuffle=False)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.15/0.85, random_state=42, shuffle=False)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_valid = torch.tensor(X_valid.values, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid.values, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Create the train, validation, and test datasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "# Define the data loaders\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model, criterion, optimizer, and scheduler\n",
    "model = TransformerModel(n_features=X.shape[1])\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, scheduler):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "    return train_loss / len(train_loader.dataset)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "\n",
    "def validate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "    return val_loss / len(val_loader.dataset)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train_loss = train(model, train_loader, criterion, optimizer, scheduler)\n",
    "    val_loss = validate(model, val_loader, criterion)\n",
    "    print(f'Epoch [{epoch}/{n_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss = validate(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
