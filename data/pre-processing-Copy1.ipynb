{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5315d982",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39efd413",
   "metadata": {},
   "source": [
    "## A. Data Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d452011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import utils\n",
    "from scipy.stats import skew\n",
    "import math\n",
    "import re\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80116c66",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 17 fields in line 14, saw 18\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# load the data set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/ibulmnie/Documents/GitHub/ML20222.PredictionBitcoin/ML20222.PredictionBitcoin/data/raw_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:812\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:873\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:848\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:859\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:2025\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 17 fields in line 14, saw 18\n"
     ]
    }
   ],
   "source": [
    "# load the data set\n",
    "df = pd.read_csv(\"/Users/ibulmnie/Documents/GitHub/ML20222.PredictionBitcoin/ML20222.PredictionBitcoin/data/raw_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2dcfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2fffec3",
   "metadata": {},
   "source": [
    "Check the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, cols = df.shape\n",
    "print(\"There are {} rows.\".format(rows))\n",
    "print(\"There are {} columns.\".format(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bed342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data type\n",
    "df.info(verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3381eec2",
   "metadata": {},
   "source": [
    "## B. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32097d89",
   "metadata": {},
   "source": [
    "This section focuses on enhancing the data quality by finding the caveats of the data set and manipulating and dropping data before feeding them into the model's input pipeline. \n",
    "It also involves feature selection methods aimed at eliminating features that have the least impact on the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73bfd5a",
   "metadata": {},
   "source": [
    "## 1. Evaluating Target Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbc55cb",
   "metadata": {},
   "source": [
    "Since the objective is to predict the price of Bitcoin, which is represented by the \"BTC_close\" variable, it is essential to ensure that \"BTC_close\" is valid for all records. \n",
    "Otherwise, if there are missing or invalid values in the \"BTC_close\" variable, it would result in the absence of a target variable for the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8855ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['BTC_close'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865cf2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of records with NULL price: \", df['BTC_close'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0956192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect the row indices where pice is NULL\n",
    "price_isna_idx = np.where(df['BTC_close'].isna()==True)[0]\n",
    "# remove the rows where target price is not available \n",
    "df.drop(price_isna_idx, axis=0, inplace=True)\n",
    "print(\"Number of records with NULL price: \", df['BTC_close'].isna().sum())\n",
    "print(\"-\"*50)\n",
    "rows, cols = df.shape\n",
    "print(\"There are {} rows.\".format(rows))\n",
    "print(\"There are {} columns.\".format(cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b04f034",
   "metadata": {},
   "source": [
    "Before clipping the values of the target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e481280",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "price_dist = sns.displot(df['BTC_close'], kde=True)\n",
    "price_dist.fig.set_size_inches(8,5)\n",
    "print(\"Skewness: \", df['BTC_close'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a62ed2f",
   "metadata": {},
   "source": [
    "?????? Các giá trị ngoại lai có thể mang thông tin quan trọng hoặc đại diện cho các biến động đặc biệt trong thị trường Bitcoin. Nếu xóa các giá trị này, có thể mất đi một phần quan trọng của thông tin và dẫn đến kết quả dự đoán không chính xác. => không xoá nhỉ\n",
    "Lý do: \n",
    "1. Quan sát biến động cao: Giá Bitcoin có thể biến động mạnh và có những giai đoạn tăng giá đáng kể. Các giá trị ngoại lai có thể đại diện cho những biến động này và mang thông tin quan trọng về sự biến động của thị trường Bitcoin. Bằng cách giữ lại các giá trị ngoại lai, bạn có thể cho phép mô hình học hỏi và ứng phó với những biến động tương tự trong tương lai.\n",
    "\n",
    "2. Quan tâm đến sự biến đổi đặc biệt: Các giá trị ngoại lai có thể đại diện cho các sự kiện đặc biệt hoặc các yếu tố độc đáo trong thị trường Bitcoin. Bằng cách giữ lại các giá trị này, bạn có thể cho phép mô hình nhận biết và tìm hiểu về các yếu tố đặc biệt này, giúp cải thiện khả năng dự đoán trong tương lai.\n",
    "\n",
    "3. Tiềm năng lợi nhuận cao: Vì giá Bitcoin có thể tăng lên rất cao, việc giữ lại các giá trị ngoại lai có thể mang lại tiềm năng lợi nhuận cao trong trường hợp mô hình dự đoán chính xác các biến động này. Nếu bạn xóa các giá trị ngoại lai, bạn có thể mất đi cơ hội tận dụng những biến động giá trị Bitcoin tiềm năng.\n",
    "\n",
    "4. Quan sát các xu hướng dài hạn: Bằng cách giữ lại các giá trị ngoại lai, bạn có thể giúp mô hình nhận biết và quan sát các xu hướng dài hạn của giá Bitcoin. Điều này có thể cung cấp thông tin giá trị về sự phát triển và tiềm năng tương lai của Bitcoin, giúp cải thiện khả năng dự đoán.\n",
    "\n",
    "5. Tính chất đặc biệt của Bitcoin: Bitcoin có tính chất độc đáo và có thể trải qua các biến động rất lớn trong giá trị. Điều này có thể dẫn đến sự lệch trong biểu đồ khi các giá trị ngoại lai biểu thị những biến động này. Biểu đồ lệch có thể phản ánh tính chất không thường xuyên và không đồng nhất của dữ liệu Bitcoin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b14d5e",
   "metadata": {},
   "source": [
    "## 2. Handling NULL Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfb3989",
   "metadata": {},
   "source": [
    "#### a. Dealing with the datetime feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2dfe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Date' column to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'], dayfirst=True, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb95307",
   "metadata": {},
   "source": [
    "#### b. NULL values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc67183e",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_count_arr = df.isna().sum()\n",
    "print(null_count_arr.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c003da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of missing values by 24 features \n",
    "null_ratio = (null_count_arr.sort_values(ascending=False)[:24]/df.shape[0])*100\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.xticks(rotation='vertical')\n",
    "sns.barplot(x = null_ratio.index, y = null_ratio)\n",
    "plt.xlabel('Features', fontsize = 15)\n",
    "plt.ylabel('Missing Values (%)', fontsize = 5)\n",
    "plt.title('Percentage of Missing Values by Features', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e5437",
   "metadata": {},
   "source": [
    "By observing the above information, we realize there are two problems with NULL data.\n",
    "\n",
    "Firstly: Market Index including (S&P500, DJI, JP225) and Commodities (Gold, Silver, Copper) have no transactions on Saturday, and Sunday. If you look at our data set, they are all NULL on Saturdays and Sundays.\n",
    "\n",
    "----> Solution: fill in the null value of Saturday, Sunday using the Friday of the last trading session of that week\n",
    "\n",
    "Second: Other Cryptocurrencies (DOGE, LTC, ETH, XRP) have a NULL price because it is newly released later, specifically as follows:\n",
    "XRP : 22/01/2015\n",
    "ETH : 10/03/2016\n",
    "LTC : April 28, 2016\n",
    "DOGE : 03/06/2017\n",
    "----> Solution: fill null value with 0 representing the non-occurrence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51b2d3c",
   "metadata": {},
   "source": [
    "#### The first problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5adcd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the earliest date by excluding missing values\n",
    "earliest_date = df['Date'].dropna().min()\n",
    "\n",
    "# Iterate over each row in the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    date = row['Date']\n",
    "    # Check if the date value is missing (NaN)\n",
    "    if pd.isnull(date):\n",
    "        # Assign the earliest date to the missing value\n",
    "        df.at[index, 'Date'] = earliest_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3818556",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6dba20",
   "metadata": {},
   "source": [
    "#### The second problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86517101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fill in the missing data with 0 value\n",
    "df_filled = df.fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6457cf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existence of NaN value in DataFrame\n",
    "print(\"Is There any 'NaN' value: \", df_filled.isnull().values.any())\n",
    "\n",
    "# Check existence of duplicate values in DataFrame\n",
    "print(\"Is there any duplicate value: \", df_filled.index.duplicated().any())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe542e9",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec08c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = df.set_index('Date') \n",
    "sns.heatmap(ds.corr(), cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfbdda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the number of categorical fields and numerical fields\n",
    "# field indices where column is categorical\n",
    "categorical_col_idx = np.where(ds.dtypes == object)[0]\n",
    "# field indices where column is numerical\n",
    "numerical_col_idx = np.where(ds.dtypes != object)[0]\n",
    "print(\"Number of categorical fields: \",categorical_col_idx.shape[0])\n",
    "print(\"Number of numerical fields: \",numerical_col_idx.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9929e733",
   "metadata": {},
   "source": [
    "## 4. Finding outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b3ec00",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = df.drop(\"Date\", axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33a089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in continuous_features:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[feature] = np.log1p(df_copy[feature])\n",
    "    df_copy.boxplot(column=feature)\n",
    "    plt.ylabel(feature)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889170e6",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c2b683",
   "metadata": {},
   "source": [
    "#### a. Preparing X and y to do train test split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39614a5b",
   "metadata": {},
   "source": [
    "This part is for task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc5bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_filled.iloc[:, 1:]\n",
    "y = df_filled.iloc[:, 0]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8807c007",
   "metadata": {},
   "source": [
    "#### b. The sample of the training and test files for the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d10fccc",
   "metadata": {},
   "source": [
    "###### IF the way you divide is to use the date call to meet the set ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c146ce64",
   "metadata": {},
   "source": [
    "Train, Test = 85, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e24116",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from datetime import datetime\n",
    "\n",
    "for index in total.index:\n",
    "    total.loc[index, \"Date\"] = datetime.strptime(str(total.loc[index, \"Date\"])[:10], '%Y-%m-%d').date()\n",
    "\n",
    "Start_day = date(2016, 3, 10)\n",
    "Test_day = date(2022,3,1)\n",
    "End_day = date(2023,4,16)\n",
    "\n",
    "\n",
    "# train, test\n",
    "total = total[(total[\"Date\"] >= Start_day) & (total[\"Date\"] <= End_day)].reset_index(drop = True)\n",
    "train_dataset = total[total[\"Date\"] < Test_day].reset_index(drop = True)\n",
    "test_dataset = total[total[\"Date\"] >= Test_day].reset_index(drop = True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5a62f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train = train_dataset.drop([\"Date\"], axis=1)[:-1]\n",
    "y_train = train_dataset[\"BTC_close\"][1:].reset_index(drop=True)\n",
    "\n",
    "X_test = test_dataset.drop([\"Date\"], axis=1)[:-1]\n",
    "y_test = test_dataset[\"BTC_close\"][1:].reset_index(drop=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0f64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"test_ratio = len(test_dataset) / len(total)\n",
    "\n",
    "print(\"Test_data/total:\", test_ratio)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f899c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"print(\"X_train size:\", X_train.shape)\n",
    "print(\"Y_train size:\", y_train.shape)\n",
    "print(\"X_test size:\", X_test.shape)\n",
    "print(\"Size y_test:\", y_test.shape)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d051e3cd",
   "metadata": {},
   "source": [
    "###### IF the way you split is to use train_test_split from Sklearn library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760b8661",
   "metadata": {},
   "source": [
    "Train, Valid, Test = 70,15,15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8559cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"from sklearn.model_selection import train_test_split\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a20344",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"X_train, X_test, y_train, y_testn = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, shuffle=False)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c7e68a",
   "metadata": {},
   "source": [
    "## 6. Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff1b303",
   "metadata": {},
   "source": [
    "Using the \"Extra Trees Regressor\" tool of the Scikitlearn library to create a graph that visually shows the importance of the independent variables X to the dependent variable Y in the forecasting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d963639",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "feat_selection = ExtraTreesRegressor()\n",
    "feat_selection.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074ebf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(feat_selection.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2c817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a graph of feature importances\n",
    "feat_importances = pd.Series(feat_selection.feature_importances_, index=X.columns)\n",
    "feat_importances.nlargest(23).plot(kind='barh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab2b871",
   "metadata": {},
   "source": [
    "But for our case the number of features are less and our purpose so we will use all of them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10c3dd7",
   "metadata": {},
   "source": [
    "## 7. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134381a9",
   "metadata": {},
   "source": [
    "## C. Finishing Touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62564fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data in .csv format for model building\n",
    "#df_filled.to_csv('data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
